{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a65cae4-b60a-484b-93d7-1d142dceba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextBox, LTTextLine\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Type, Literal \n",
    "from pydantic import BaseModel\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import json\n",
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "from glob import glob\n",
    "from IPython.display import HTML, Markdown, Image \n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4aa26b6e-9f1e-4cf2-a20f-cc2690451eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = r'C:\\Users\\Akshaya V\\git\\Earnings research\\Earnings_agent\\PublicReportResearch-main'\n",
    "full_path = os.path.join(dic, 'docs')\n",
    "question=\"Extract the net revenue of citi bank\"\n",
    "llm_model_name=\"qwen3:4b\"\n",
    "embedding_model=\"nomic-embed-text\"\n",
    "sec_excel_path=r'C:/Users/Akshaya V/git/Earnings research/Earnings_agent/PublicReportResearch-main/50_metrics.xlsx'\n",
    "sec_excel=pd.read_excel(r'C:/Users/Akshaya V/git/Earnings research/Earnings_agent/PublicReportResearch-main/50_metrics.xlsx')\n",
    "bank_name_mapping = {'AMERICAN EXPRESS COMPANY': 'American Express',\n",
    "    'Bank of America Corporation': 'Bank of America',\n",
    "    'CAPITAL\\xa0ONE\\xa0FINANCIAL\\xa0CORP': 'Capital One',\n",
    "    'Citigroup\\xa0Inc': 'Citi',\n",
    "    'Fifth Third Bancorp': 'Fifth Third',\n",
    "    'Huntington Bancshares Incorporated': 'Huntington Bank',\n",
    "    'JPMorgan Chase & Co': 'JPMorgan Chase',\n",
    "    'KeyCorp': 'KeyBank',\n",
    "    'NORTHERN TRUST CORPORATION': 'Northern Trust',\n",
    "    'PNC Financial Services Group, Inc.': 'PNC Bank',\n",
    "    \"People's United Financial, Inc.\": 'Peoples United',\n",
    "    'SCHWAB CHARLES CORP': 'Charles Schwab',\n",
    "    'STATE STREET CORPORATION': 'State Street',\n",
    "    'TEGNA INC.': 'Tegna',\n",
    "    'THE BANK OF NEW YORK MELLON CORPORATION': 'BNY Mellon',\n",
    "    'TRUIST FINANCIAL CORPORATION': 'Truist',\n",
    "    'The Goldman Sachs Group, Inc.': 'Goldman Sachs',\n",
    "    'US BANCORP \\\\DE\\\\': 'US Bancorp',\n",
    "    'WELLS FARGO & COMPANY/MN': 'Wells Fargo'}\n",
    "\n",
    "sec_excel['CompanyName']=sec_excel['CompanyName'].replace(bank_name_mapping)\n",
    "allowed_metrics: List[str] = sec_excel.columns[2:].unique().tolist()\n",
    "allowed_banks: List[str] = sec_excel['CompanyName'].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa3aad2-7aa5-4963-a81f-31a528ab6184",
   "metadata": {},
   "source": [
    "**Intent of the question - Exact or vague**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f975dfce-f3d0-4579-8ac3-6f64bea7f353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. Define the Pydantic Schema\n",
    "# ---------------------------\n",
    "class ParsedRequest_intent(BaseModel):\n",
    "    intent: str                 \n",
    " \n",
    "\n",
    "# ---------------------------\n",
    "# 2. Function to Use ChatOllama with Pydantic Schema\n",
    "# ---------------------------\n",
    "def intent(\n",
    "    llm_model_name: str,\n",
    "    user_input: str,\n",
    "    schema: Type[BaseModel]\n",
    "    \n",
    ") -> BaseModel:\n",
    "    system_prompt = \"\"\"You are a expert in classifying questions into 2 categories. The 2 categories are exact question , needs_clarification. A question is marked as exact if it has concrete details in 3 categories - company name, quarter & Year , metrics to be analysed. It is needs_clarification if the user uses words like analyse, in detail , research, elaborate or if the user doesn't provide specific metrics or company name or year & quarter to be analysed . give the output in JSON format srtictly . JSON has one key and it is called intent . For example ) {\"intent\": \"exact\"}\"\"\"\n",
    "    # Load the Ollama model\n",
    "    llm = ChatOllama(model=llm_model_name)\n",
    "\n",
    "    # Create and send the prompt\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt.strip()),\n",
    "        HumanMessage(content=user_input.strip())\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content.strip()\n",
    "    # Validate using Pydantic\n",
    "    # try:\n",
    "    #     raw = response.content.strip()\n",
    "    \n",
    "    #     # Optional: clean triple backticks if LLM returns markdown\n",
    "    #     if \"```\" in raw:\n",
    "    #         import re\n",
    "    #         match = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", raw, re.DOTALL)\n",
    "    #         if match:\n",
    "    #             raw = match.group(1)\n",
    "    \n",
    "    #     # Step 1: Convert JSON string to Python dict\n",
    "    #     parsed_dict = json.loads(raw)\n",
    "    \n",
    "    #     # Step 2: Validate and convert into a Pydantic object\n",
    "    #     parsed_model = schema.model_validate(parsed_dict)\n",
    "    \n",
    "    #     # Step 3: Return it (now it has `.model_dump_json()` etc.)\n",
    "    #     return parsed_model\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     raise ValueError(f\"Failed to parse model output: {e}\\nRaw Output:\\n{response.content}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4735c25c-52fb-4271-b8a5-0c38f13a2ff8",
   "metadata": {},
   "source": [
    "**REPHRASING VAGUE QUESTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9ab241c-4293-4b1b-b0f7-d773702c1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vague(model, question, allowed_banks, allowed_metrics):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in rewriting vague finance-related questions. Your sole task is to **rephrase the user's question** by **expanding it explicitly** along three dimensions:\n",
    "- Company names (banks)\n",
    "- Financial metrics\n",
    "- Quarters and Years\n",
    "\n",
    "Important:\n",
    "- DO NOT provide an answer, explanation, or rationale.\n",
    "- ONLY return the **rewritten question** in plain text.\n",
    "\n",
    "Defaults (if the user doesn’t specify):\n",
    "1. Company: Wells Fargo\n",
    "2. Quarters: 1Q2025, 4Q2024, 3Q2024\n",
    "3. Metrics: NetIncome, EarningsPerShare, TotalRevenue\n",
    "4. Add important metrics for senior leadership (e.g., ReturnOnEquity, ROA, CET1Ratio)\n",
    "\n",
    "Rules:\n",
    "- Use only official bank names from: {json.dumps(allowed_banks)}\n",
    "- Convert quarters to format: \"1Q2025\"\n",
    "- Use only metrics from: {json.dumps(allowed_metrics)}\n",
    "\n",
    "Example:\n",
    "User: Extract the net revenue of citi bank in 2025Q1  \n",
    "Output: Extract TotalRevenue, NetIncome, EarningsPerShare, and ReturnOnEquity for Citigroup Inc in 1Q2025, 4Q2024, and 3Q2024.\n",
    "\n",
    "Respond ONLY with the rewritten version of the user’s question.\n",
    "Here is the question:  {question}\n",
    "    \"\"\"\n",
    "\n",
    "    llm = ChatOllama(model=model)\n",
    "   # print(prompt)\n",
    "   \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0422107-ecab-4ed3-ac0f-398144d79423",
   "metadata": {},
   "source": [
    "**Getting Pydantic schema inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd92e029-2e60-43e6-b0fa-6f0f027d7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. Define the Pydantic Schema\n",
    "# ---------------------------\n",
    "class ParsedRequest(BaseModel):\n",
    "                 \n",
    "    banks: List[str]            # Must match allowed_banks\n",
    "    quarters: List[str]         # e.g., \"1Q2025\", \"4Q2024\"\n",
    "    metrics: List[str]          # Must match allowed_metrics\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Function to Use ChatOllama with Pydantic Schema\n",
    "# ---------------------------\n",
    "def parse_with_chatollama(\n",
    "        llm_model_name: str,\n",
    "        user_input: str,\n",
    "        schema: Type[BaseModel],\n",
    "        allowed_banks: List[str],\n",
    "        allowed_metrics: List[str]\n",
    "    ) -> BaseModel:\n",
    "    system_prompt = f\"\"\"\n",
    "You are a financial assistant. Your task is to extract structured information from user input and return it in the following JSON format:\n",
    "\n",
    "{{\n",
    "  \n",
    "  \"banks\": [valid bank names],\n",
    "  \"quarters\": [\"1Q2025\", \"4Q2024\", \"3Q2024\"],\n",
    "  \"metrics\": [valid metric keys]\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- Map any abbreviation or alias to official bank names from this list: {json.dumps(allowed_banks)}\n",
    "- Extract all mentioned quarters in \"1Q2025\" format. Include the previous 2 quarters for each.\n",
    "- Extract only metrics listed here: {json.dumps(allowed_metrics)}.\n",
    "- Output only the JSON structure as shown above, no explanation or markdown.\n",
    "\"\"\"\n",
    "\n",
    "    # Load the Ollama model\n",
    "    llm = ChatOllama(model=llm_model_name)\n",
    "\n",
    "    # Create and send the prompt\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt.strip()),\n",
    "        HumanMessage(content=user_input.strip())\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content.strip()\n",
    "\n",
    "    # Validate using Pydantic\n",
    "    # try:\n",
    "    #     raw = response.content.strip()\n",
    "    \n",
    "    #     # Optional: clean triple backticks if LLM returns markdown\n",
    "    #     if \"```\" in raw:\n",
    "    #         import re\n",
    "    #         match = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", raw, re.DOTALL)\n",
    "    #         if match:\n",
    "    #             raw = match.group(1)\n",
    "    \n",
    "    #     # Step 1: Convert JSON string to Python dict\n",
    "    #     parsed_dict = json.loads(raw)\n",
    "    \n",
    "    #     # Step 2: Validate and convert into a Pydantic object\n",
    "    #     parsed_model = schema.model_validate(parsed_dict)\n",
    "    \n",
    "   #     # Step 3: Return it (now it has `.model_dump_json()` etc.)\n",
    "    #     return parsed_model\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     raise ValueError(f\"Failed to parse model output: {e}\\nRaw Output:\\n{response.content}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df4bda-1f90-4500-aa71-81f5d093202b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2cca8dc-ae8d-452e-a8cb-184e6594f8d4",
   "metadata": {},
   "source": [
    "**PDF Reader RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bd79d4e-15f6-4f87-9420-6de47bc5c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParsedRequest(BaseModel):\n",
    "    intent: str\n",
    "    banks: List[str]\n",
    "    quarters: List[str]\n",
    "    metrics: List[str]\n",
    "\n",
    "def is_natural_language(text):\n",
    "    return bool(re.search(r\"[A-Za-z]{4,}.*\\.\", text)) and not is_table_like(text)\n",
    "\n",
    "def is_table_like(text):\n",
    "    lines = text.strip().splitlines()\n",
    "    if len(lines) < 2:\n",
    "        return False\n",
    "\n",
    "    table_like = 0\n",
    "    for line in lines:\n",
    "        tokens = line.strip().split()\n",
    "        num_tokens = len(tokens)\n",
    "        numbers = len([t for t in tokens if re.fullmatch(r\"[\\d,.%$]+\", t)])\n",
    "        symbols = len([t for t in tokens if re.fullmatch(r\"[\\d,.%$O/(U)-]+\", t)])\n",
    "\n",
    "        if num_tokens >= 3 and numbers / num_tokens > 0.5:\n",
    "            table_like += 1\n",
    "        elif len(re.findall(r\"\\$\\s?\\d\", line)) > 1:\n",
    "            table_like += 1\n",
    "        elif len(re.findall(r\"\\d{2,},\", line)) > 1:\n",
    "            table_like += 1\n",
    "\n",
    "    return table_like / len(lines) > 0.4\n",
    "\n",
    "def extract_text_excluding_tables(pdf_path):\n",
    "    final_text = []\n",
    "    for page_layout in extract_pages(pdf_path):\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, (LTTextBox, LTTextLine)):\n",
    "                text = element.get_text().strip()\n",
    "                if text and is_natural_language(text):\n",
    "                    final_text.append(text)\n",
    "    return \"\\n\\n\".join(final_text).strip()\n",
    "\n",
    "def parse_filename_metadata(filename: str):\n",
    "    name = os.path.splitext(os.path.basename(filename))[0]\n",
    "    parts = name.split(\"_\")\n",
    "    bank_map = {\n",
    "        \"jpm\": \"JP Morgan Chase\",\n",
    "        \"boa\": \"Bank of America\",\n",
    "        \"citi\": \"Citigroup\",\n",
    "        \"gs\": \"Goldman Sachs\",\n",
    "        \"ms\": \"Morgan Stanley\",\n",
    "    }\n",
    "    bank_code = parts[0].lower()\n",
    "    quarter = parts[1].upper() if len(parts) > 1 else \"UNKNOWN\"\n",
    "    bank = bank_map.get(bank_code, bank_code.upper())\n",
    "    return bank, quarter\n",
    "\n",
    "def create_chunks_with_ollama(text: str, metadata: dict = None):\n",
    "    embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    chunker = SemanticChunker(embeddings=embedder, min_chunk_size=2000)\n",
    "    doc = Document(page_content=text, metadata=metadata or {}\n",
    "    return chunker.split_documents([doc])\n",
    "\n",
    "def search_chunks(chunks, parsed_query: ParsedRequest, top_k=5, index_path=\"faiss_index\"):\n",
    "    embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "    # Step 1: Filter by metadata\n",
    "    filter_dict = {\n",
    "        \"bank\": parsed_query.banks,\n",
    "        \"quarter\": parsed_query.quarters\n",
    "    }\n",
    "\n",
    "    # Step 2: Load or create FAISS vectorstore\n",
    "    if os.path.exists(index_path):\n",
    "        vectorstore = FAISS.load_local(index_path, embedder, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        vectorstore = FAISS.from_documents(chunks, embedder)\n",
    "        vectorstore.save_local(index_path)\n",
    "\n",
    "    # Step 3: Search\n",
    "    query_text = (\n",
    "        f\"Find information about {', '.join(parsed_query.metrics)} \"\n",
    "        f\"for banks like {', '.join(parsed_query.banks)} \"\n",
    "        f\"during quarters such as {', '.join(parsed_query.quarters)}\"\n",
    "    )\n",
    "\n",
    "    return vectorstore.similarity_search(query_text, k=top_k, filter=filter_dict)\n",
    "\n",
    "def rerank_with_chatollama(chunks, parsed_query: ParsedRequest):\n",
    "    llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "    context = \"\\n\\n\".join([chunk.page_content for chunk in chunks])\n",
    "    prompt = (\n",
    "        f\"You are a financial analyst assistant.\\\n\\n\"\n",
    "        f\"Query:\\\n{parsed_query.model_dump_json(indent=2)}\\
\\n\"\n",
    "        f\"Extracted text:\\\n{context}\\
\\n\"\n",
    "        f\"Please summarize the relevant details in a table or paragraph based on the query intent.\"\n",
    "    )\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "for pdf in os.listdir(full_path):\n",
    "    if not pdf.lower().endswith(\".pdf\"):\n",
    "        continue\n",
    "\n",
    "    pdf_path = os.path.join(full_path, pdf)\n",
    "    print(f\" Processing: {pdf}\")\n",
    "\n",
    "    bank, quarter = parse_filename_metadata(pdf)\n",
    "    clean_text = extract_text_excluding_tables(pdf_path)\n",
    "\n",
    "    tagged_text = f\"<{bank} {quarter}>\\n{clean_text}\\n</{bank} {quarter}>\"\n",
    "    metadata = {\"source\": pdf, \"bank\": bank, \"quarter\": quarter}\n",
    "    chunks = create_chunks_with_ollama(tagged_text, metadata)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"\\nTotal Chunks Created: {len(all_chunks)}\")\n",
    "for i, chunk in enumerate(all_chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(\"Metadata:\", chunk.metadata)\n",
    "    print(\"Content preview:\", chunk.page_content[:300], \"...\\n\")\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "530eb228-c219-45c1-a50b-f8e813bca1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "# vectorstore = FAISS.load_local(\"faiss_index\", embedder, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5db33e6-e5aa-4837-9f2d-d65e3cb70d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d807c6f-699e-4b90-93b0-d353cfd28b9f",
   "metadata": {},
   "source": [
    "**EXCEL EXTRACTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0a432c7-04c0-40b9-b0c1-30a54e8cf785",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Input Schema for Tool ---\n",
    "class FinancialDataInput(BaseModel):\n",
    "    banks: List[str]\n",
    "    quarters: List[str]\n",
    "    metrics: List[str]\n",
    "\n",
    "# --- Helper Function ---\n",
    "def convert_to_qtr(date_str):\n",
    "    \"\"\"Convert a date to '1Q2025' style format.\"\"\"\n",
    "    date = pd.to_datetime(date_str)\n",
    "    quarter = (date.month - 1) // 3 + 1\n",
    "    return f\"{quarter}Q{date.year}\"\n",
    "\n",
    "# --- Tool Definition ---\n",
    "@tool\n",
    "def extract_bank_metrics(input_data: FinancialDataInput) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts filtered financial data for selected banks, quarters, and metrics from the Excel file.\n",
    "    \"\"\"\n",
    "    excel_path = sec_excel_path  # <-- Replace this with your actual file path\n",
    "    df = pd.read_excel(excel_path)\n",
    "    bank_name_mapping = {'AMERICAN EXPRESS COMPANY': 'American Express',\n",
    "    'Bank of America Corporation': 'Bank of America',\n",
    "    'CAPITAL\\xa0ONE\\xa0FINANCIAL\\xa0CORP': 'Capital One',\n",
    "    'Citigroup\\xa0Inc': 'Citi',\n",
    "    'Fifth Third Bancorp': 'Fifth Third',\n",
    "    'Huntington Bancshares Incorporated': 'Huntington Bank',\n",
    "    'JPMorgan Chase & Co': 'JPMorgan Chase',\n",
    "    'KeyCorp': 'KeyBank',\n",
    "    'NORTHERN TRUST CORPORATION': 'Northern Trust',\n",
    "    'PNC Financial Services Group, Inc.': 'PNC Bank',\n",
    "    \"People's United Financial, Inc.\": 'Peoples United',\n",
    "    'SCHWAB CHARLES CORP': 'Charles Schwab',\n",
    "    'STATE STREET CORPORATION': 'State Street',\n",
    "    'TEGNA INC.': 'Tegna',\n",
    "    'THE BANK OF NEW YORK MELLON CORPORATION': 'BNY Mellon',\n",
    "    'TRUIST FINANCIAL CORPORATION': 'Truist',\n",
    "    'The Goldman Sachs Group, Inc.': 'Goldman Sachs',\n",
    "    'US BANCORP \\\\DE\\\\': 'US Bancorp',\n",
    "    'WELLS FARGO & COMPANY/MN': 'Wells Fargo'}\n",
    "\n",
    "    df['CompanyName']=df['CompanyName'].replace(bank_name_mapping)\n",
    "    df['Datetime']=pd.to_datetime(df['Datetime'],format=\"%Y-%m-%d\")\n",
    "    df['Quarter']=df['Datetime'].apply(convert_to_qtr)\n",
    "    print(df.head())\n",
    "\n",
    "    banks = input_data.banks\n",
    "    quarters = input_data.quarters\n",
    "    metrics = input_data.metrics\n",
    "\n",
    "    result = df.loc[(df['CompanyName'].str.upper().isin([b.upper() for b in banks])) & (df['Quarter'].isin(quarters).values)]\n",
    "    if result.empty:\n",
    "        return {\"message\":\"No data found\"}\n",
    "    print(result) \n",
    "    response={\"CompanyName\":result['CompanyName'].values,\n",
    "              \"Quarter\":quarters,}\n",
    "    for metric in metrics:\n",
    "        response[metric]=result.iloc[0].get(metric,\"metric not found\")\n",
    "    result=result[['CompanyName','Datetime']+metrics]\n",
    "    result['Datetime']=result['Datetime'].apply(convert_to_qtr)\n",
    "    response=result.to_json(orient='records')\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f58cb-8fc3-4513-a3a0-aaee6c409580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Input Schema for Chart Tool ---\n",
    "class ChartInput(BaseModel):\n",
    "    data: List[dict]\n",
    "    chart_type: Literal['bar', 'line']\n",
    "    title: str\n",
    "    x_axis: str\n",
    "    y_axis: List[str]\n",
    "\n",
    "# --- Charting Tool Definition ---\n",
    "@tool\n",
    "def create_chart(input_data: ChartInput) -> str:\n",
    "    \"\"\"\n",
    "    Generates a chart from the provided data and returns the path to the saved chart image.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(input_data.data)\n",
    "    \n",
    "    if df.empty:\n",
    "        return \"No data provided to generate chart.\"\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if input_data.chart_type == 'bar':\n",
    "        df.plot(x=input_data.x_axis, y=input_data.y_axis, kind='bar')\n",
    "    elif input_data.chart_type == 'line':\n",
    "        df.plot(x=input_data.x_axis, y=input_data.y_axis, kind='line')\n",
    "\n",
    "    plt.title(input_data.title)\n",
    "    plt.xlabel(input_data.x_axis)\n",
    "    plt.ylabel(', '.join(input_data.y_axis))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    chart_path = f\"{input_data.title.replace(' ', '_')}.png\"\n",
    "    plt.savefig(chart_path)\n",
    "    plt.close()\n",
    "    \n",
    "    return f\"Chart saved to {chart_path}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0a432c7-04c0-40b9-b0c1-30a54e8cf785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structured_input = FinancialDataInput(**input_params)\n",
    "\n",
    "# \ud83d\udd27 Call your tool (directly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6e2baf8-0ecd-4768-9216-e7188a3f969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_json = structured_query_json\n",
    "# input1=(json.loads(structured_query_json))\n",
    "# print(type(input1))\n",
    "# print(type(input1.get('banks')))\n",
    "# excel_path = sec_excel_path\n",
    "# output = get_financial_data(excel_path, input1)\n",
    "\n",
    "# # Pretty print\n",
    "# table_output=json.dumps(output, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2092f57c-7e5b-46ab-a37e-4225c7c40abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba2bb150-354a-4804-a273-7a90ec80643e",
   "metadata": {},
   "source": [
    "**Final Table and Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77196bf5-e9fe-481e-9b3e-1bdaf07a15ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_detailed_html_report(llm_model_name: str, table_output: dict, extracted_text: str,question:str,intent:str) -> str:\n",
    "    llm = ChatOllama(model=llm_model_name)\n",
    "    exact_prompt=f\"\"\" You are provided with a json{table_output}. Convert that to html format by adding tags and display it as a table \n",
    "User question :{question}\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a professional financial analyst creating an internal report for Wells Fargo senior leadership.\n",
    "Your response must contain three clearly formatted parts in HTML TAgs with embedded CSS to make it visually appealing and boardroom-ready.\n",
    "\n",
    "### Instructions:\n",
    "1. **Part 1**: Present the input JSON request as a table and add HTML Table tags (no changes in values).\n",
    "2. **Part 2**: Use the extracted text to create a detailed narrative summary based on the below question asked. Structure this as a qualitative analysis that highlights trends, anomalies, risks, and growth areas.\n",
    "3. **Part 3**: Ensure the writing style is impressive to senior executives — use formal, insightful, and concise language.\n",
    "\n",
    "### Formatting Rules:\n",
    "- Use <table>, <thead>, <tbody>, <tr>, <th>, <td> for tables\n",
    "- Use <p>, <h2>, <h3> for textual sections\n",
    "- Use inline CSS to match Wells Fargo branding (deep red: #b31b1b, gold: #ffd700, and professional fonts)\n",
    "- Make layout visually appealing (padding, borders, alternating row colors, aligned text)\n",
    "- Output ONLY valid HTML tags (no Markdown or commentary)\n",
    "\n",
    "---\n",
    "\n",
    "JSON Request:\n",
    "```json\n",
    "{table_output}\n",
    "```\n",
    "\n",
    "Extracted Report Text:\n",
    "{extracted_text}\n",
    "\n",
    "User question :\n",
    "{question}\n",
    "\"\"\"\n",
    "    if intent==\"exact\":\n",
    "        final_question=exact_prompt\n",
    "    else:\n",
    "        final_question=prompt\n",
    "    response = llm.invoke([HumanMessage(content=final_question.strip())])\n",
    "    return response.content\n",
    "\n",
    "# Example usage\n",
    "# html_output = generate_financial_html_response(\"qwen2.5:7b\", parsed_query, extracted_text)\n",
    "# display(HTML(html_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "752e8625-4c83-4ad1-842f-27ad2dfa99c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example parsed_query dict\n",
    "# input_params={'banks': ['Citi'], 'quarters': ['1Q2025', '4Q2024', '3Q2024'], 'metrics': ['TotalRevenue', 'NetIncome', 'EarningsPerShare', 'ReturnOnEquity']}\n",
    "# parsed_query = input_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a5406-26a3-4289-8ba6-558a472a6b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54b57a-e911-430b-af8a-7483199d8172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c041e32-8f0a-427d-8e92-541a3c2f21b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " intetnt: {'intent': 'needs_clarification'}\n",
      "\n",
      " question: Extract TotalRevenue, NetIncome, EarningsPerShare, and ReturnOnEquity for Citigroup Inc in 1Q2025, 4Q2024, and 3Q2024.\n"
     ]
    }
   ],
   "source": [
    "result = intent(\n",
    "    llm_model_name=llm_model_name,  # Replace with your loaded Ollama model name\n",
    "    user_input=question,\n",
    "    schema=ParsedRequest_intent\n",
    ")\n",
    "intent1 = result.split(\"</think>\")[-1].strip()\n",
    "intent=(json.loads(intent1))\n",
    "#intent=intent[\"intent\"]\n",
    "print(\"\\n intetnt:\", intent)\n",
    "\n",
    "############\n",
    "if (intent['intent']!=\"exact\"):\n",
    "    result = vague(\n",
    "    model=llm_model_name,  # Replace with your loaded Ollama model name\n",
    "    question=question,\n",
    "    allowed_banks=allowed_banks,\n",
    "    allowed_metrics=allowed_metrics\n",
    "    )\n",
    "    question = result.split(\"</think>\")[-1].strip()\n",
    "    print(\"\\n question:\",question)\n",
    "#############\n",
    "\n",
    "result = parse_with_chatollama(\n",
    "    llm_model_name=llm_model_name,  # Replace with your loaded Ollama model name\n",
    "    user_input=question,\n",
    "    schema=ParsedRequest,\n",
    "    allowed_banks=allowed_banks,\n",
    "    allowed_metrics=allowed_metrics\n",
    ")\n",
    "response = result.split(\"</think>\")[-1].strip()\n",
    "print(\"\\n Pydantic schema: \",response)\n",
    "###################\n",
    "# response=(result.model_dump_json())\n",
    "input_params=(json.loads(response))\n",
    "print(input_params)\n",
    "\n",
    "\n",
    "\n",
    "###################\n",
    "\n",
    "intent_ext=intent['intent']\n",
    "\n",
    "# Prepare structured query\n",
    "#print(intent, input_params)\n",
    "structured_query = {**intent_ext, **input_params}\n",
    "print(\"\\n structured query:\",structured_query)\n",
    "\n",
    "structured_query_json = json.dumps(structured_query, indent=2)\n",
    "print(\"\\n structured query:\",structured_query_json)\n",
    "\n",
    "parsed_query = ParsedRequest.model_validate(json.loads(structured_query_json))\n",
    "\n",
    "# Load or create FAISS index\n",
    "index_path = \"faiss_index\"\n",
    "embedder = OllamaEmbeddings(model=embedding_model)\n",
    "\n",
    "if os.path.exists(index_path):\n",
    "    print(\"Loading FAISS index from disk...\")\n",
    "    vectorstore = FAISS.load_local(index_path, embedder, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    print(\"No FAISS index found. Creating new one from all_chunks...\")\n",
    "    vectorstore = FAISS.from_documents(all_chunks, embedder)\n",
    "    vectorstore.save_local(index_path)\n",
    "    print(\"Saved FAISS index to disk.\")\n",
    "\n",
    "# Build semantic search query\n",
    "query_text = (\n",
    "    f\"Find information about {', '.join(parsed_query.metrics)} \"\n",
    "    f\"for banks like {', '.join(parsed_query.banks)} \"\n",
    "    f\"during quarters such as {', '.join(parsed_query.quarters)}\"\n",
    ")\n",
    "\n",
    "# Construct metadata filter for FAISS\n",
    "filter_dict = {\n",
    "    \"bank\": parsed_query.banks,\n",
    "    \"quarter\": parsed_query.quarters\n",
    "}\n",
    "\n",
    "matched_chunks = vectorstore.similarity_search(query_text, k=5, filter=filter_dict)\n",
    "\n",
    "# Output matched content with metadata\n",
    "final_ans = \"\"\n",
    "for i, chunk in enumerate(matched_chunks):\n",
    "    metadata = chunk.metadata\n",
    "    print(f\"\\n--- Matched Chunk {i+1} ---\")\n",
    "    print(f\" Source: {metadata.get('source', 'N/A')}\")\n",
    "    print(f\" Bank: {metadata.get('bank', 'Unknown')}\")\n",
    "    print(f\" Quarter: {metadata.get('quarter', 'Unknown')}\")\n",
    "    print(\" Content Preview:\\n\", chunk.page_content[:300], \"...\\n\")\n",
    "    \n",
    "    final_ans += (\n",
    "        f\"<div class='chunk'>\\n\"\n",
    "        f\"<p><strong>Source:</strong> {metadata.get('source', 'N/A')}</p>\\n\"\n",
    "        f\"<p><strong>Bank:</strong> {metadata.get('bank', 'Unknown')}</p>\\n\"\n",
    "        f\"<p><strong>Quarter:</strong> {metadata.get('quarter', 'Unknown')}</p>\\n\"\n",
    "        f\"<p>{chunk.page_content}</p>\\n\"\n",
    "        f\"</div>\\n\\n\"\n",
    "    )\n",
    "\n",
    "print(\" Final Combined Answer with Metadata:\\n\")\n",
    "print(final_ans)\n",
    "\n",
    "\n",
    "#####################\n",
    "excel_Data = extract_bank_metrics.invoke({\"input_data\":input_params})\n",
    "#structured_input)\n",
    "table_output=json.dumps(excel_Data, indent=2)\n",
    "print(\" \\n Table output\",table_output)\n",
    "\n",
    "\n",
    "#####################\n",
    "intent=intent['intent']\n",
    "# Example: Create a string of matched chunk content\n",
    "\n",
    "\n",
    "# Call function\n",
    "html_output = generate_detailed_html_report(llm_model_name, table_output, final_ans, question,intent)\n",
    "\n",
    "# Optionally write to HTML file\n",
    "display(HTML(html_output))\n",
    "\n",
    "#####################\n",
    "# Example of how to call the charting tool\n",
    "if excel_Data and excel_Data != '{\"message\":\"No data found\"}':\n",
    "    chart_input = {\n",
    "        \"data\": json.loads(excel_Data),\n",
    "        \"chart_type\": \"bar\",\n",
    "        \"title\": f\"Financial Metrics for {', '.join(input_params['banks'])}\",\n",
    "        \"x_axis\": \"Datetime\",\n",
    "        \"y_axis\": input_params['metrics']\n",
    "    }\n",
    "    chart_path = create_chart.invoke({\"input_data\": chart_input})\n",
    "    print(chart_path)\n",
    "    display(Image(filename=chart_path.split(' ')[-1]))\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0eaa7f-e354-45b7-acc6-329bdad77ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1=\"Identify any extraordinary or one time items only for 1Q2025 in the earnings release and supplemetn. quantify their impact on the following where disclosed - net income, total non interest income, total non interest expense , net income available to common (NIAC), earnings per share (EPS) both diluted and basic\"\n",
    "prompt2=\"Summarizt the key drivers of the banks 1Q2025 financial performance . for each of the following - net interest income , non interest income and expense. - quantify the change vs 1Q2024 and 1Q2024 in both dollar terms and percentage terms (Explain the primary causes) \"\n",
    "prompt3=\"Fee based income composition analyse the composition of fee based income of 1Q2025 . For each major category - Example Asset management, investment banking, card fees, service charges) . Identigy the contribution to total fee income. Highlight any categories with noticible increases or dclines vs 4Q2024 and 1Q2024 \"\n",
    "prompt4=\"Net interst margin (NIM) breakdown where possible break down the net interest margin by loan and deposit type (eg commercial, residential , consumer loans, demand deposit , time deposit ). For each, show the change in NIM in basis points between 1Q2025 and 4Q2024 . Assess whether changes were due to inteest rate shifts, funding cost pressure or loan/deposit mix\"\n",
    "prompt5=\"Expense composition analysis - analyzr the composition of total expense in 1Q2025 vs 4Q2024. Disaggregate by key categories . example. compensation and benefit , technology, occupation). Identiy which categories contributed most to the change in total expenses\"\n",
    "prompt6= \"Loan portfolo changes - provide an analysis of changes in loan portfolio balances. SEgment by loan type (example commercial, residentital mortgages m credit card etc) . Highlight changes in origination and or deliquencies between 1Q2025 , 4Q2024 and 1Q2024 \"\n",
    "prompt7=\"Credit quality metrics - evaluate trends in credit quality metrtics between 1Q2025, 4Q2024 and 1Q2024 including provision for credit losses , net charge off . Break fown by loan category and highlight any areas with deterioration or increased risk. Quantify material changes and indicate whether they appear cyclic or structural\"\n",
    "prompt8=\"Forward looking statement & strategic initiatives - identify anyy forward looking statement or strategic initiatives mentioned in 1Q2025 . Sumamrize updates to management guidance (Example reveneue outlook, NIM expectations , cost initiatives, capital return plans) . Highlight commentary on macro trends (Ex. rate outlook, credit conditions) that may affect 2025 performance \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e216a93-108d-4c92-ba5d-08bf492c864e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65cae4-b60a-484b-93d7-1d142dceba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextBox, LTTextLine\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Type, Literal \n",
    "from pydantic import BaseModel\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import json\n",
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "from glob import glob\n",
    "from IPython.display import HTML, Markdown \n",
    "from pydantic import BaseModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa26b6e-9f1e-4cf2-a20f-cc2690451eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = r'C:\\Users\\Akshaya V\\git\\Earnings research\\Earnings_agent\\PublicReportResearch-main'\n",
    "full_path = os.path.join(dic, 'docs')\n",
    "question=\"Extract the net revenue of citi bank\"\n",
    "llm_model_name=\"qwen3:4b\"\n",
    "embedding_model=\"nomic-embed-text\"\n",
    "sec_excel_path=r'C:/Users/Akshaya V/git/Earnings research/Earnings_agent/PublicReportResearch-main/50_metrics.xlsx'\n",
    "sec_excel=pd.read_excel(r'C:/Users/Akshaya V/git/Earnings research/Earnings_agent/PublicReportResearch-main/50_metrics.xlsx')\n",
    "bank_name_mapping = {'AMERICAN EXPRESS COMPANY': 'American Express',\n",
    "    'Bank of America Corporation': 'Bank of America',\n",
    "    'CAPITAL\\xa0ONE\\xa0FINANCIAL\\xa0CORP': 'Capital One',\n",
    "    'Citigroup\\xa0Inc': 'Citi',\n",
    "    'Fifth Third Bancorp': 'Fifth Third',\n",
    "    'Huntington Bancshares Incorporated': 'Huntington Bank',\n",
    "    'JPMorgan Chase & Co': 'JPMorgan Chase',\n",
    "    'KeyCorp': 'KeyBank',\n",
    "    'NORTHERN TRUST CORPORATION': 'Northern Trust',\n",
    "    'PNC Financial Services Group, Inc.': 'PNC Bank',\n",
    "    \"People's United Financial, Inc.\": 'Peoples United',\n",
    "    'SCHWAB CHARLES CORP': 'Charles Schwab',\n",
    "    'STATE STREET CORPORATION': 'State Street',\n",
    "    'TEGNA INC.': 'Tegna',\n",
    "    'THE BANK OF NEW YORK MELLON CORPORATION': 'BNY Mellon',\n",
    "    'TRUIST FINANCIAL CORPORATION': 'Truist',\n",
    "    'The Goldman Sachs Group, Inc.': 'Goldman Sachs',\n",
    "    'US BANCORP \\\\DE\\\\': 'US Bancorp',\n",
    "    'WELLS FARGO & COMPANY/MN': 'Wells Fargo'}\n",
    "\n",
    "sec_excel['CompanyName']=sec_excel['CompanyName'].replace(bank_name_mapping)\n",
    "allowed_metrics: List[str] = sec_excel.columns[2:].unique().tolist()\n",
    "allowed_banks: List[str] = sec_excel['CompanyName'].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa3aad2-7aa5-4963-a81f-31a528ab6184",
   "metadata": {},
   "source": [
    "**Intent of the question - Exact or vague**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975dfce-f3d0-4579-8ac3-6f64bea7f353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. Define the Pydantic Schema\n",
    "# ---------------------------\n",
    "class ParsedRequest_intent(BaseModel):\n",
    "    intent: str                 \n",
    " \n",
    "\n",
    "# ---------------------------\n",
    "# 2. Function to Use ChatOllama with Pydantic Schema\n",
    "# ---------------------------\n",
    "def intent(\n",
    "    llm_model_name: str,\n",
    "    user_input: str,\n",
    "    schema: Type[BaseModel]\n",
    "    \n",
    ") -> BaseModel:\n",
    "    system_prompt = \"\"\"You are a expert in classifying questions into 2 categories. The 2 categories are exact question , needs_clarification. A question is marked as exact if it has concrete details in 3 categories - company name, quarter & Year , metrics to be analysed. It is needs_clarification if the user uses words like analyse, in detail , research, elaborate or if the user doesn't provide specific metrics or company name or year & quarter to be analysed . give the output in JSON format srtictly . JSON has one key and it is called intent . For example ) {\"intent\": \"exact\"}\"\"\"\n",
    "    # Load the Ollama model\n",
    "    llm = ChatOllama(model=llm_model_name)\n",
    "\n",
    "    # Create and send the prompt\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt.strip()),\n",
    "        HumanMessage(content=user_input.strip())\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content.strip()\n",
    "    # Validate using Pydantic\n",
    "    # try:\n",
    "    #     raw = response.content.strip()\n",
    "    \n",
    "    #     # Optional: clean triple backticks if LLM returns markdown\n",
    "    #     if \"```\" in raw:\n",
    "    #         import re\n",
    "    #         match = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", raw, re.DOTALL)\n",
    "    #         if match:\n",
    "    #             raw = match.group(1)\n",
    "    \n",
    "    #     # Step 1: Convert JSON string to Python dict\n",
    "    #     parsed_dict = json.loads(raw)\n",
    "    \n",
    "    #     # Step 2: Validate and convert into a Pydantic object\n",
    "    #     parsed_model = schema.model_validate(parsed_dict)\n",
    "    \n",
    "    #     # Step 3: Return it (now it has `.model_dump_json()` etc.)\n",
    "    #     return parsed_model\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     raise ValueError(f\"Failed to parse model output: {e}\\nRaw Output:\\n{response.content}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "result = intent(\n",
    "    llm_model_name=llm_model_name,  # Replace with your loaded Ollama model name\n",
    "    user_input=question,\n",
    "    schema=ParsedRequest_intent\n",
    ")\n",
    "intent1 = result.split(\"</think>\")[-1].strip()\n",
    "intent=(json.loads(intent1))\n",
    "#intent=intent[\"intent\"]\n",
    "print(intent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4735c25c-52fb-4271-b8a5-0c38f13a2ff8",
   "metadata": {},
   "source": [
    "**REPHRASING VAGUE QUESTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab241c-4293-4b1b-b0f7-d773702c1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vague(model, question, allowed_banks, allowed_metrics):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in rewriting vague finance-related questions. Your sole task is to **rephrase the user's question** by **expanding it explicitly** along three dimensions:\n",
    "- Company names (banks)\n",
    "- Financial metrics\n",
    "- Quarters and Years\n",
    "\n",
    "Important:\n",
    "- DO NOT provide an answer, explanation, or rationale.\n",
    "- ONLY return the **rewritten question** in plain text.\n",
    "\n",
    "Defaults (if the user doesn‚Äôt specify):\n",
    "1. Company: Wells Fargo\n",
    "2. Quarters: 1Q2025, 4Q2024, 3Q2024\n",
    "3. Metrics: NetIncome, EarningsPerShare, TotalRevenue\n",
    "4. Add important metrics for senior leadership (e.g., ReturnOnEquity, ROA, CET1Ratio)\n",
    "\n",
    "Rules:\n",
    "- Use only official bank names from: {json.dumps(allowed_banks)}\n",
    "- Convert quarters to format: \"1Q2025\"\n",
    "- Use only metrics from: {json.dumps(allowed_metrics)}\n",
    "\n",
    "Example:\n",
    "User: Extract the net revenue of citi bank in 2025Q1  \n",
    "Output: Extract TotalRevenue, NetIncome, EarningsPerShare, and ReturnOnEquity for Citigroup Inc in 1Q2025, 4Q2024, and 3Q2024.\n",
    "\n",
    "Respond ONLY with the rewritten version of the user‚Äôs question.\n",
    "Here is the question:  {question}\n",
    "    \"\"\"\n",
    "\n",
    "    llm = ChatOllama(model=model)\n",
    "   # print(prompt)\n",
    "   \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content.strip()\n",
    "\n",
    "if (intent!=\"exact\"):\n",
    "    result = vague(\n",
    "    model=llm_model_name,  # Replace with your loaded Ollama model name\n",
    "    question=question,\n",
    "    allowed_banks=allowed_banks,\n",
    "    allowed_metrics=allowed_metrics\n",
    "    )\n",
    "    question = result.split(\"</think>\")[-1].strip()\n",
    "    print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0422107-ecab-4ed3-ac0f-398144d79423",
   "metadata": {},
   "source": [
    "**Getting Pydantic schema inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd92e029-2e60-43e6-b0fa-6f0f027d7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. Define the Pydantic Schema\n",
    "# ---------------------------\n",
    "class ParsedRequest(BaseModel):\n",
    "                 \n",
    "    banks: List[str]            # Must match allowed_banks\n",
    "    quarters: List[str]         # e.g., \"1Q2025\", \"4Q2024\"\n",
    "    metrics: List[str]          # Must match allowed_metrics\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Function to Use ChatOllama with Pydantic Schema\n",
    "# ---------------------------\n",
    "def parse_with_chatollama(\n",
    "        llm_model_name: str,\n",
    "        user_input: str,\n",
    "        schema: Type[BaseModel],\n",
    "        allowed_banks: List[str],\n",
    "        allowed_metrics: List[str]\n",
    "    ) -> BaseModel:\n",
    "    system_prompt = f\"\"\"\n",
    "You are a financial assistant. Your task is to extract structured information from user input and return it in the following JSON format:\n",
    "\n",
    "{{\n",
    "  \n",
    "  \"banks\": [valid bank names],\n",
    "  \"quarters\": [\"1Q2025\", \"4Q2024\", \"3Q2024\"],\n",
    "  \"metrics\": [valid metric keys]\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- Map any abbreviation or alias to official bank names from this list: {json.dumps(allowed_banks)}\n",
    "- Extract all mentioned quarters in \"1Q2025\" format. Include the previous 2 quarters for each.\n",
    "- Extract only metrics listed here: {json.dumps(allowed_metrics)}.\n",
    "- Output only the JSON structure as shown above, no explanation or markdown.\n",
    "\"\"\"\n",
    "\n",
    "    # Load the Ollama model\n",
    "    llm = ChatOllama(model=llm_model_name)\n",
    "\n",
    "    # Create and send the prompt\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt.strip()),\n",
    "        HumanMessage(content=user_input.strip())\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content.strip()\n",
    "\n",
    "    # Validate using Pydantic\n",
    "    # try:\n",
    "    #     raw = response.content.strip()\n",
    "    \n",
    "    #     # Optional: clean triple backticks if LLM returns markdown\n",
    "    #     if \"```\" in raw:\n",
    "    #         import re\n",
    "    #         match = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", raw, re.DOTALL)\n",
    "    #         if match:\n",
    "    #             raw = match.group(1)\n",
    "    \n",
    "    #     # Step 1: Convert JSON string to Python dict\n",
    "    #     parsed_dict = json.loads(raw)\n",
    "    \n",
    "    #     # Step 2: Validate and convert into a Pydantic object\n",
    "    #     parsed_model = schema.model_validate(parsed_dict)\n",
    "    \n",
    "   #     # Step 3: Return it (now it has `.model_dump_json()` etc.)\n",
    "    #     return parsed_model\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     raise ValueError(f\"Failed to parse model output: {e}\\nRaw Output:\\n{response.content}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df4bda-1f90-4500-aa71-81f5d093202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = parse_with_chatollama(\n",
    "    llm_model_name=llm_model_name,  # Replace with your loaded Ollama model name\n",
    "    user_input=question,\n",
    "    schema=ParsedRequest,\n",
    "    allowed_banks=allowed_banks,\n",
    "    allowed_metrics=allowed_metrics\n",
    ")\n",
    "response = result.split(\"</think>\")[-1].strip()\n",
    "print(response)\n",
    "\n",
    "# response=(result.model_dump_json())\n",
    "input_params=(json.loads(response))\n",
    "print(input_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cca8dc-ae8d-452e-a8cb-184e6594f8d4",
   "metadata": {},
   "source": [
    "**PDF Reader RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd79d4e-15f6-4f87-9420-6de47bc5c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParsedRequest(BaseModel):\n",
    "    intent: str\n",
    "    banks: List[str]\n",
    "    quarters: List[str]\n",
    "    metrics: List[str]\n",
    "\n",
    "def is_natural_language(text):\n",
    "    return bool(re.search(r\"[A-Za-z]{4,}.*\\.\", text)) and not is_table_like(text)\n",
    "\n",
    "def is_table_like(text):\n",
    "    lines = text.strip().splitlines()\n",
    "    if len(lines) < 2:\n",
    "        return False\n",
    "\n",
    "    table_like = 0\n",
    "    for line in lines:\n",
    "        tokens = line.strip().split()\n",
    "        num_tokens = len(tokens)\n",
    "        numbers = len([t for t in tokens if re.fullmatch(r\"[\\d,.%$]+\", t)])\n",
    "        symbols = len([t for t in tokens if re.fullmatch(r\"[\\d,.%$O/(U)-]+\", t)])\n",
    "\n",
    "        if num_tokens >= 3 and numbers / num_tokens > 0.5:\n",
    "            table_like += 1\n",
    "        elif len(re.findall(r\"\\$\\s?\\d\", line)) > 1:\n",
    "            table_like += 1\n",
    "        elif len(re.findall(r\"\\d{2,},\", line)) > 1:\n",
    "            table_like += 1\n",
    "\n",
    "    return table_like / len(lines) > 0.4\n",
    "\n",
    "def extract_text_excluding_tables(pdf_path):\n",
    "    final_text = []\n",
    "    for page_layout in extract_pages(pdf_path):\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, (LTTextBox, LTTextLine)):\n",
    "                text = element.get_text().strip()\n",
    "                if text and is_natural_language(text):\n",
    "                    final_text.append(text)\n",
    "    return \"\\n\\n\".join(final_text).strip()\n",
    "\n",
    "def parse_filename_metadata(filename: str):\n",
    "    name = os.path.splitext(os.path.basename(filename))[0]\n",
    "    parts = name.split(\"_\")\n",
    "    bank_map = {\n",
    "        \"jpm\": \"JP Morgan Chase\",\n",
    "        \"boa\": \"Bank of America\",\n",
    "        \"citi\": \"Citigroup\",\n",
    "        \"gs\": \"Goldman Sachs\",\n",
    "        \"ms\": \"Morgan Stanley\",\n",
    "    }\n",
    "    bank_code = parts[0].lower()\n",
    "    quarter = parts[1].upper() if len(parts) > 1 else \"UNKNOWN\"\n",
    "    bank = bank_map.get(bank_code, bank_code.upper())\n",
    "    return bank, quarter\n",
    "\n",
    "def create_chunks_with_ollama(text: str, metadata: dict = None):\n",
    "    embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    chunker = SemanticChunker(embeddings=embedder, min_chunk_size=2000)\n",
    "    doc = Document(page_content=text, metadata=metadata or {})\n",
    "    return chunker.split_documents([doc])\n",
    "\n",
    "def search_chunks(chunks, parsed_query: ParsedRequest, top_k=5, index_path=\"faiss_index\"):\n",
    "    embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "    # Step 1: Filter by metadata\n",
    "    filtered_chunks = [\n",
    "        chunk for chunk in chunks\n",
    "        if chunk.metadata.get(\"bank\") in parsed_query.banks and chunk.metadata.get(\"quarter\") in parsed_query.quarters\n",
    "    ]\n",
    "\n",
    "    # Step 2: Load or create FAISS vectorstore\n",
    "    if os.path.exists(index_path):\n",
    "        vectorstore = load_faiss_index(index_path)\n",
    "    else:\n",
    "        vectorstore = FAISS.from_documents(filtered_chunks, embedder)\n",
    "        save_faiss_index(vectorstore, index_path)\n",
    "\n",
    "    # Step 3: Search\n",
    "    query_text = (\n",
    "        f\"Find information about {', '.join(parsed_query.metrics)} \"\n",
    "        f\"for banks like {', '.join(parsed_query.banks)} \"\n",
    "        f\"during quarters such as {', '.join(parsed_query.quarters)}\"\n",
    "    )\n",
    "\n",
    "    return vectorstore.similarity_search(query_text, k=top_k)\n",
    "\n",
    "def rerank_with_chatollama(chunks, parsed_query: ParsedRequest):\n",
    "    llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "    context = \"\\n\\n\".join([chunk.page_content for chunk in chunks])\n",
    "    prompt = (\n",
    "        f\"You are a financial analyst assistant.\\n\\n\"\n",
    "        f\"Query:\\n{parsed_query.model_dump_json(indent=2)}\\n\\n\"\n",
    "        f\"Extracted text:\\n{context}\\n\\n\"\n",
    "        f\"Please summarize the relevant details in a table or paragraph based on the query intent.\"\n",
    "    )\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "# all_chunks = []\n",
    "\n",
    "# for pdf in os.listdir(full_path):\n",
    "#     if not pdf.lower().endswith(\".pdf\"):\n",
    "#         continue\n",
    "\n",
    "#     pdf_path = os.path.join(full_path, pdf)\n",
    "#     print(f\" Processing: {pdf}\")\n",
    "\n",
    "#     bank, quarter = parse_filename_metadata(pdf)\n",
    "#     clean_text = extract_text_excluding_tables(pdf_path)\n",
    "\n",
    "#     tagged_text = f\"<{bank} {quarter}>\\n{clean_text}\\n</{bank} {quarter}>\"\n",
    "#     metadata = {\"source\": pdf, \"bank\": bank, \"quarter\": quarter}\n",
    "#     chunks = create_chunks_with_ollama(tagged_text, metadata)\n",
    "#     all_chunks.extend(chunks)\n",
    "\n",
    "# print(f\"\\nTotal Chunks Created: {len(all_chunks)}\")\n",
    "# for i, chunk in enumerate(all_chunks[:3]):\n",
    "#     print(f\"\\n--- Chunk {i+1} ---\")\n",
    "#     print(\"Metadata:\", chunk.metadata)\n",
    "#     print(\"Content preview:\", chunk.page_content[:300], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530eb228-c219-45c1-a50b-f8e813bca1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "# vectorstore = FAISS.load_local(\"faiss_index\", embedder, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5db33e6-e5aa-4837-9f2d-d65e3cb70d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "#intent=\"exact\"\n",
    "structured_query={'intent': 'needs_clarification', 'banks': ['Citi'], 'quarters': ['1Q2025', '4Q2024', '3Q2024'], 'metrics': ['TotalRevenue', 'NetIncome', 'EarningsPerShare', 'ReturnOnEquity']}\n",
    "# Prepare structured query\n",
    "#print(intent, input_params)\n",
    "#structured_query = {**intent, **input_params}\n",
    "print(structured_query)\n",
    "\n",
    "structured_query_json = json.dumps(structured_query, indent=2)\n",
    "print(structured_query_json)\n",
    "\n",
    "parsed_query = ParsedRequest.model_validate(json.loads(structured_query_json))\n",
    "\n",
    "# Load or create FAISS index\n",
    "index_path = \"faiss_index\"\n",
    "embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "if os.path.exists(index_path):\n",
    "    print(\"üîÅ Loading FAISS index from disk...\")\n",
    "    vectorstore = FAISS.load_local(index_path, embedder, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    print(\"‚öôÔ∏è No FAISS index found. Creating new one from all_chunks...\")\n",
    "    vectorstore = FAISS.from_documents(all_chunks, embedder)\n",
    "    vectorstore.save_local(index_path)\n",
    "    print(\"‚úÖ Saved FAISS index to disk.\")\n",
    "\n",
    "# Build semantic search query\n",
    "query_text = (\n",
    "    f\"Find information about {', '.join(parsed_query.metrics)} \"\n",
    "    f\"for banks like {', '.join(parsed_query.banks)} \"\n",
    "    f\"during quarters such as {', '.join(parsed_query.quarters)}\"\n",
    ")\n",
    "\n",
    "matched_chunks = vectorstore.similarity_search(query_text, k=5)\n",
    "\n",
    "# Output matched content with metadata\n",
    "final_ans = \"\"\n",
    "for i, chunk in enumerate(matched_chunks):\n",
    "    metadata = chunk.metadata\n",
    "    print(f\"\\n--- Matched Chunk {i+1} ---\")\n",
    "    print(f\"üìÑ Source: {metadata.get('source', 'N/A')}\")\n",
    "    print(f\"üè¶ Bank: {metadata.get('bank', 'Unknown')}\")\n",
    "    print(f\"üìÖ Quarter: {metadata.get('quarter', 'Unknown')}\")\n",
    "    print(\"üß† Content Preview:\\n\", chunk.page_content[:300], \"...\\n\")\n",
    "    \n",
    "    final_ans += (\n",
    "        f\"<div class='chunk'>\\n\"\n",
    "        f\"<p><strong>Source:</strong> {metadata.get('source', 'N/A')}</p>\\n\"\n",
    "        f\"<p><strong>Bank:</strong> {metadata.get('bank', 'Unknown')}</p>\\n\"\n",
    "        f\"<p><strong>Quarter:</strong> {metadata.get('quarter', 'Unknown')}</p>\\n\"\n",
    "        f\"<p>{chunk.page_content}</p>\\n\"\n",
    "        f\"</div>\\n\\n\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Final Combined Answer with Metadata:\\n\")\n",
    "print(final_ans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d807c6f-699e-4b90-93b0-d353cfd28b9f",
   "metadata": {},
   "source": [
    "**EXCEL EXTRACTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a432c7-04c0-40b9-b0c1-30a54e8cf785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "# --- Input Schema for Tool ---\n",
    "class FinancialDataInput(BaseModel):\n",
    "    banks: List[str]\n",
    "    quarters: List[str]\n",
    "    metrics: List[str]\n",
    "\n",
    "# --- Helper Function ---\n",
    "def convert_to_qtr(date_str):\n",
    "    \"\"\"Convert a date to '1Q2025' style format.\"\"\"\n",
    "    date = pd.to_datetime(date_str)\n",
    "    quarter = (date.month - 1) // 3 + 1\n",
    "    return f\"{quarter}Q{date.year}\"\n",
    "\n",
    "# --- Tool Definition ---\n",
    "@tool\n",
    "def extract_bank_metrics(input_data: FinancialDataInput) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts filtered financial data for selected banks, quarters, and metrics from the Excel file.\n",
    "    \"\"\"\n",
    "    excel_path = sec_excel_path  # <-- Replace this with your actual file path\n",
    "    df = pd.read_excel(excel_path)\n",
    "    bank_name_mapping = {'AMERICAN EXPRESS COMPANY': 'American Express',\n",
    "    'Bank of America Corporation': 'Bank of America',\n",
    "    'CAPITAL\\xa0ONE\\xa0FINANCIAL\\xa0CORP': 'Capital One',\n",
    "    'Citigroup\\xa0Inc': 'Citi',\n",
    "    'Fifth Third Bancorp': 'Fifth Third',\n",
    "    'Huntington Bancshares Incorporated': 'Huntington Bank',\n",
    "    'JPMorgan Chase & Co': 'JPMorgan Chase',\n",
    "    'KeyCorp': 'KeyBank',\n",
    "    'NORTHERN TRUST CORPORATION': 'Northern Trust',\n",
    "    'PNC Financial Services Group, Inc.': 'PNC Bank',\n",
    "    \"People's United Financial, Inc.\": 'Peoples United',\n",
    "    'SCHWAB CHARLES CORP': 'Charles Schwab',\n",
    "    'STATE STREET CORPORATION': 'State Street',\n",
    "    'TEGNA INC.': 'Tegna',\n",
    "    'THE BANK OF NEW YORK MELLON CORPORATION': 'BNY Mellon',\n",
    "    'TRUIST FINANCIAL CORPORATION': 'Truist',\n",
    "    'The Goldman Sachs Group, Inc.': 'Goldman Sachs',\n",
    "    'US BANCORP \\\\DE\\\\': 'US Bancorp',\n",
    "    'WELLS FARGO & COMPANY/MN': 'Wells Fargo'}\n",
    "\n",
    "    df['CompanyName']=df['CompanyName'].replace(bank_name_mapping)\n",
    "    df['Datetime']=pd.to_datetime(df['Datetime'],format=\"%Y-%m-%d\")\n",
    "    df['Quarter']=df['Datetime'].apply(convert_to_qtr)\n",
    "    print(df.head())\n",
    "\n",
    "    banks = input_data.banks\n",
    "    quarters = input_data.quarters\n",
    "    metrics = input_data.metrics\n",
    "\n",
    "    result = df.loc[(df['CompanyName'].str.upper().isin([b.upper() for b in banks])) & (df['Quarter'].isin(quarters).values)]\n",
    "    if result.empty:\n",
    "        return {\"message\":\"No data found\"}\n",
    "    print(result) \n",
    "    response={\"CompanyName\":result['CompanyName'].values,\n",
    "              \"Quarter\":quarters,}\n",
    "    for metric in metrics:\n",
    "        response[metric]=result.iloc[0].get(metric,\"metric not found\")\n",
    "    result=result[['CompanyName','Datetime']+metrics]\n",
    "    result['Datetime']=result['Datetime'].apply(convert_to_qtr)\n",
    "    response=result.to_json(orient='records')\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a35f58cb-8fc3-4513-a3a0-aaee6c409580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Datetime       CompanyName     NetIncome  TotalRevenue  NetInterestIncome  \\\n",
      "0 2019-03-31  American Express  1.550000e+09  6.697000e+09       2.059000e+09   \n",
      "1 2019-06-30  American Express  3.311000e+09  1.377600e+10       4.133000e+09   \n",
      "2 2019-09-30  American Express  5.066000e+09  2.082700e+10       6.336000e+09   \n",
      "3 2019-12-31  American Express  6.759000e+09  2.815900e+10       8.620000e+09   \n",
      "4 2020-03-31  American Express  3.670000e+08  6.296000e+09       2.330000e+09   \n",
      "\n",
      "   ReturnOnEquity  ReturnOnAssets  EarningsPerShare  EarningsPerShareDiluted  \\\n",
      "0        6.976326        0.786032              1.81                     1.80   \n",
      "1       14.338299        1.675582              3.88                     3.87   \n",
      "2       22.002172        2.608866              5.97                     5.95   \n",
      "3       29.296519        3.408111              8.00                     7.99   \n",
      "4        1.747120        0.197248              0.41                     0.41   \n",
      "\n",
      "    TotalAssets  ...  TotalCapitalRatio  RiskWeightedAssets  \\\n",
      "0  1.971930e+11  ...                NaN                 NaN   \n",
      "1  1.976030e+11  ...                NaN                 NaN   \n",
      "2  1.941840e+11  ...                NaN                 NaN   \n",
      "3  1.983210e+11  ...                NaN                 NaN   \n",
      "4  1.860600e+11  ...                NaN                 NaN   \n",
      "\n",
      "   LeverageCapitalRatio  LiquidityRatio  TradingAssets  \\\n",
      "0                   NaN             NaN            NaN   \n",
      "1                   NaN             NaN            NaN   \n",
      "2                   NaN             NaN            NaN   \n",
      "3                   NaN             NaN            NaN   \n",
      "4                   NaN             NaN            NaN   \n",
      "\n",
      "   AvailableForSaleSecurities  HeldToMaturitySecurities      Goodwill  \\\n",
      "0                6.410000e+09                       NaN           NaN   \n",
      "1                8.457000e+09                       NaN           NaN   \n",
      "2                8.370000e+09                       NaN           NaN   \n",
      "3                8.328000e+09                       NaN  3.315000e+09   \n",
      "4                4.957000e+09                       NaN           NaN   \n",
      "\n",
      "   IntangibleAssets  Quarter  \n",
      "0               NaN   1Q2019  \n",
      "1               NaN   2Q2019  \n",
      "2               NaN   3Q2019  \n",
      "3       267000000.0   4Q2019  \n",
      "4               NaN   1Q2020  \n",
      "\n",
      "[5 rows x 55 columns]\n",
      "     Datetime CompanyName     NetIncome  TotalRevenue  NetInterestIncome  \\\n",
      "96 2024-09-30        Citi  9.826000e+09  6.155800e+10       4.036200e+10   \n",
      "97 2024-12-31        Citi  1.268200e+10  8.113900e+10       5.409500e+10   \n",
      "98 2025-03-31        Citi  4.064000e+09  2.159600e+10       1.401200e+10   \n",
      "\n",
      "    ReturnOnEquity  ReturnOnAssets  EarningsPerShare  EarningsPerShareDiluted  \\\n",
      "96        4.699569        0.404252              4.67                     4.61   \n",
      "97        6.079636        0.538984              6.03                     5.94   \n",
      "98        1.913299        0.158039              2.00                     1.96   \n",
      "\n",
      "     TotalAssets  ...  TotalCapitalRatio  RiskWeightedAssets  \\\n",
      "96  2.430663e+12  ...                NaN                 NaN   \n",
      "97  2.352945e+12  ...                NaN                 NaN   \n",
      "98  2.571514e+12  ...                NaN                 NaN   \n",
      "\n",
      "    LeverageCapitalRatio  LiquidityRatio  TradingAssets  \\\n",
      "96                   NaN             NaN   4.580720e+11   \n",
      "97                   NaN             NaN   4.427470e+11   \n",
      "98                   NaN             NaN   5.185770e+11   \n",
      "\n",
      "    AvailableForSaleSecurities  HeldToMaturitySecurities      Goodwill  \\\n",
      "96                         NaN                       NaN  1.969100e+10   \n",
      "97                         NaN                       NaN  1.930000e+10   \n",
      "98                         NaN                       NaN  1.942200e+10   \n",
      "\n",
      "    IntangibleAssets  Quarter  \n",
      "96      4.121000e+09   3Q2024  \n",
      "97      4.494000e+09   4Q2024  \n",
      "98      4.430000e+09   1Q2025  \n",
      "\n",
      "[3 rows x 55 columns]\n",
      "\"[{\\\"CompanyName\\\":\\\"Citi\\\",\\\"Datetime\\\":\\\"3Q2024\\\",\\\"TotalRevenue\\\":61558000000.0,\\\"NetIncome\\\":9826000000.0,\\\"EarningsPerShare\\\":4.67,\\\"ReturnOnEquity\\\":4.6995690707},{\\\"CompanyName\\\":\\\"Citi\\\",\\\"Datetime\\\":\\\"4Q2024\\\",\\\"TotalRevenue\\\":81139000000.0,\\\"NetIncome\\\":12682000000.0,\\\"EarningsPerShare\\\":6.03,\\\"ReturnOnEquity\\\":6.0796364299},{\\\"CompanyName\\\":\\\"Citi\\\",\\\"Datetime\\\":\\\"1Q2025\\\",\\\"TotalRevenue\\\":21596000000.0,\\\"NetIncome\\\":4064000000.0,\\\"EarningsPerShare\\\":2.0,\\\"ReturnOnEquity\\\":1.9132989341}]\"\n"
     ]
    }
   ],
   "source": [
    "# structured_input = FinancialDataInput(**input_params)\n",
    "\n",
    "# üîß Call your tool (directly)\n",
    "excel_Data = extract_bank_metrics.invoke({\"input_data\":input_params})\n",
    "#structured_input)\n",
    "print(json.dumps(excel_Data, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6e2baf8-0ecd-4768-9216-e7188a3f969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# input_json = structured_query_json\n",
    "# input1=(json.loads(structured_query_json))\n",
    "# print(type(input1))\n",
    "# print(type(input1.get('banks')))\n",
    "# excel_path = sec_excel_path\n",
    "# output = get_financial_data(excel_path, input1)\n",
    "\n",
    "# # Pretty print\n",
    "# table_output=json.dumps(output, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2092f57c-7e5b-46ab-a37e-4225c7c40abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_output=json.dumps(excel_Data, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2bb150-354a-4804-a273-7a90ec80643e",
   "metadata": {},
   "source": [
    "**Final Table and Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77196bf5-e9fe-481e-9b3e-1bdaf07a15ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_detailed_html_report(llm_model_name: str, table_output: dict, extracted_text: str,question:str) -> str:\n",
    "    llm = ChatOllama(model=llm_model_name)\n",
    "    exact_prompt=f\"\"\" You are provided with a json{table_output}. Convert that to html format by adding tags and display it as a table \n",
    "User question :{question}\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a professional financial analyst creating an internal report for Wells Fargo senior leadership.\n",
    "Your response must contain three clearly formatted parts in HTML TAgs with embedded CSS to make it visually appealing and boardroom-ready.\n",
    "\n",
    "### Instructions:\n",
    "1. **Part 1**: Present the input JSON request as a table and add HTML Table tags (no changes in values).\n",
    "2. **Part 2**: Use the extracted text to create a detailed narrative summary based on the below question asked. Structure this as a qualitative analysis that highlights trends, anomalies, risks, and growth areas.\n",
    "3. **Part 3**: Ensure the writing style is impressive to senior executives ‚Äî use formal, insightful, and concise language.\n",
    "\n",
    "### Formatting Rules:\n",
    "- Use <table>, <thead>, <tbody>, <tr>, <th>, <td> for tables\n",
    "- Use <p>, <h2>, <h3> for textual sections\n",
    "- Use inline CSS to match Wells Fargo branding (deep red: #b31b1b, gold: #ffd700, and professional fonts)\n",
    "- Make layout visually appealing (padding, borders, alternating row colors, aligned text)\n",
    "- Output ONLY valid HTML tags (no Markdown or commentary)\n",
    "\n",
    "---\n",
    "\n",
    "JSON Request:\n",
    "```json\n",
    "{parsed_query}\n",
    "```\n",
    "\n",
    "Extracted Report Text:\n",
    "{extracted_text}\n",
    "\n",
    "User question :\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=exact_prompt.strip())])\n",
    "    return response.content\n",
    "\n",
    "# Example usage\n",
    "# html_output = generate_financial_html_response(\"qwen2.5:7b\", parsed_query, extracted_text)\n",
    "# display(HTML(html_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e8625-4c83-4ad1-842f-27ad2dfa99c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example parsed_query dict\n",
    "input_params={'banks': ['Citi'], 'quarters': ['1Q2025', '4Q2024', '3Q2024'], 'metrics': ['TotalRevenue', 'NetIncome', 'EarningsPerShare', 'ReturnOnEquity']}\n",
    "parsed_query = input_params\n",
    "\n",
    "# Example: Create a string of matched chunk content\n",
    "\n",
    "\n",
    "# Call function\n",
    "html_output = generate_detailed_html_report(llm_model_name, table_output, final_ans, question)\n",
    "\n",
    "# Optionally write to HTML file\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a5406-26a3-4289-8ba6-558a472a6b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "(html_output.split(\"</think>\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54b57a-e911-430b-af8a-7483199d8172",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c041e32-8f0a-427d-8e92-541a3c2f21b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59fb2a13-ea5b-4dd7-9ae8-015c67bd9884",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference API HTTP code: 404, {\"error\":\"model \\\"nomic-embed-text:v1.5\\\" not found, try pulling it first\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m embedding = OllamaEmbeddings(model=\u001b[33m\"\u001b[39m\u001b[33mnomic-embed-text:v1.5\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m splitter = SemanticChunker(embedding)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m split_docs = \u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 2. Vector DB\u001b[39;00m\n\u001b[32m     17\u001b[39m db = FAISS.from_documents(split_docs, embedding)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_experimental\\text_splitter.py:295\u001b[39m, in \u001b[36mSemanticChunker.split_documents\u001b[39m\u001b[34m(self, documents)\u001b[39m\n\u001b[32m    293\u001b[39m     texts.append(doc.page_content)\n\u001b[32m    294\u001b[39m     metadatas.append(doc.metadata)\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_experimental\\text_splitter.py:280\u001b[39m, in \u001b[36mSemanticChunker.create_documents\u001b[39m\u001b[34m(self, texts, metadatas)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(texts):\n\u001b[32m    279\u001b[39m     start_index = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    281\u001b[39m         metadata = copy.deepcopy(_metadatas[i])\n\u001b[32m    282\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._add_start_index:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_experimental\\text_splitter.py:228\u001b[39m, in \u001b[36mSemanticChunker.split_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    224\u001b[39m     \u001b[38;5;28mself\u001b[39m.breakpoint_threshold_type == \u001b[33m\"\u001b[39m\u001b[33mgradient\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(single_sentences_list) == \u001b[32m2\u001b[39m\n\u001b[32m    226\u001b[39m ):\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m single_sentences_list\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m distances, sentences = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_calculate_sentence_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_sentences_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.number_of_chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    230\u001b[39m     breakpoint_distance_threshold = \u001b[38;5;28mself\u001b[39m._threshold_from_clusters(distances)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_experimental\\text_splitter.py:203\u001b[39m, in \u001b[36mSemanticChunker._calculate_sentence_distances\u001b[39m\u001b[34m(self, single_sentences_list)\u001b[39m\n\u001b[32m    199\u001b[39m _sentences = [\n\u001b[32m    200\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33msentence\u001b[39m\u001b[33m\"\u001b[39m: x, \u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: i} \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(single_sentences_list)\n\u001b[32m    201\u001b[39m ]\n\u001b[32m    202\u001b[39m sentences = combine_sentences(_sentences, \u001b[38;5;28mself\u001b[39m.buffer_size)\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcombined_sentence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sentences):\n\u001b[32m    207\u001b[39m     sentence[\u001b[33m\"\u001b[39m\u001b[33mcombined_sentence_embedding\u001b[39m\u001b[33m\"\u001b[39m] = embeddings[i]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\embeddings\\ollama.py:214\u001b[39m, in \u001b[36mOllamaEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Embed documents using an Ollama deployed embedding model.\u001b[39;00m\n\u001b[32m    206\u001b[39m \n\u001b[32m    207\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    211\u001b[39m \u001b[33;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    213\u001b[39m instruction_pairs = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.embed_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction_pairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\embeddings\\ollama.py:202\u001b[39m, in \u001b[36mOllamaEmbeddings._embed\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    201\u001b[39m     iter_ = \u001b[38;5;28minput\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_emb_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\embeddings\\ollama.py:176\u001b[39m, in \u001b[36mOllamaEmbeddings._process_emb_response\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError raised by inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    177\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError raised by inference API HTTP code: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    178\u001b[39m         % (res.status_code, res.text)\n\u001b[32m    179\u001b[39m     )\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    181\u001b[39m     t = res.json()\n",
      "\u001b[31mValueError\u001b[39m: Error raised by inference API HTTP code: 404, {\"error\":\"model \\\"nomic-embed-text:v1.5\\\" not found, try pulling it first\"}"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 1. Load and split docs\n",
    "loader = TextLoader(\"citi.txt\", encoding='utf-8')\n",
    "docs = loader.load()\n",
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text:v1.5\")\n",
    "splitter = SemanticChunker(embedding)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "# 2. Vector DB\n",
    "db = FAISS.from_documents(split_docs, embedding)\n",
    "\n",
    "# 3. Base LLM\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "\n",
    "# 4. Multi-query retriever\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=db.as_retriever(), llm=llm\n",
    ")\n",
    "\n",
    "# 5. QA chain with multi-query retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=multi_query_retriever)\n",
    "\n",
    "# 6. Ask a question\n",
    "response = qa_chain.run(\"What is the EPS of citi bank in 2025 Q1?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c4f3b9-7d78-4e65-bf7d-6dad85906618",
   "metadata": {},
   "source": [
    "ðŸ“„ Text File (citi.txt)\n",
    "   â†“\n",
    "ðŸ§© Split into Chunks\n",
    "   â†“\n",
    "ðŸ”¢ Embedded into Vectors\n",
    "   â†“\n",
    "ðŸ—ƒï¸ Stored in FAISS\n",
    "   â†“\n",
    "â“ You Ask a Question\n",
    "   â†“\n",
    "ðŸ” Relevant Chunks Retrieved\n",
    "   â†“\n",
    "ðŸ§  Sent to LLM (LLaMA 3.2)\n",
    "   â†“\n",
    "ðŸ’¬ Final Answer Generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7928e6c-004b-4aa9-a75a-67763283f4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The diluted EPS (Earnings Per Share) for Citigroup Inc. in Q1 2025 was $1.96 per share.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "class MultiPromptQA:\n",
    "    def __init__(self, file_path: str, embedding_model: str, llm_model: str):\n",
    "        self.file_path = file_path\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm_model = llm_model\n",
    "\n",
    "        self.docs = self.load_and_split()\n",
    "        self.db = self.create_vectorstore()\n",
    "        self.llm = ChatOllama(model=self.llm_model)\n",
    "        self.retriever = self.create_multi_query_retriever()\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(llm=self.llm, retriever=self.retriever)\n",
    "\n",
    "    def load_and_split(self):\n",
    "        loader = TextLoader(self.file_path, encoding='utf-8')\n",
    "        docs = loader.load()\n",
    "\n",
    "        embedding = OllamaEmbeddings(model=self.embedding_model)\n",
    "        splitter = SemanticChunker(embedding)\n",
    "        split_docs = splitter.split_documents(docs)\n",
    "\n",
    "        return split_docs\n",
    "\n",
    "    def create_vectorstore(self):\n",
    "        embedding = OllamaEmbeddings(model=self.embedding_model)\n",
    "        return FAISS.from_documents(self.docs, embedding)\n",
    "\n",
    "    def create_multi_query_retriever(self):\n",
    "        return MultiQueryRetriever.from_llm(\n",
    "            retriever=self.db.as_retriever(),\n",
    "            llm=self.llm,\n",
    "            include_original=True\n",
    "        )\n",
    "\n",
    "    def ask(self, question: str) -> str:\n",
    "        return self.qa_chain.run(question)\n",
    "\n",
    "\n",
    "qa = MultiPromptQA(\n",
    "    file_path=\"C:/Users/Akshaya V/git/Earnings research/Earnings_agent/citi.txt\",\n",
    "    embedding_model=\"qwen2.5:0.5b\",\n",
    "    llm_model=\"llama3.2:3b\"\n",
    ")\n",
    "\n",
    "response = qa.ask(\"What is the EPS of citi in 2025Q1?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306b368-5784-4afc-9cd0-681d8b4064a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "class MultiPromptQA:\n",
    "    def __init__(self, file_paths: list, embedding_model: str, llm_model: str):\n",
    "        self.file_paths = file_paths\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm_model = llm_model\n",
    "\n",
    "        self.docs = self.load_and_split_all()\n",
    "        self.db = self.create_vectorstore()\n",
    "        self.llm = ChatOllama(model=self.llm_model)\n",
    "        self.retriever = self.create_multi_query_retriever()\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(llm=self.llm, retriever=self.retriever)\n",
    "\n",
    "    def load_and_split_all(self):\n",
    "        all_split_docs = []\n",
    "        embedding = OllamaEmbeddings(model=self.embedding_model)\n",
    "        splitter = SemanticChunker(embedding)\n",
    "\n",
    "        for path in self.file_paths:\n",
    "            loader = TextLoader(path, encoding='utf-8')\n",
    "            docs = loader.load()\n",
    "            split_docs = splitter.split_documents(docs)\n",
    "            all_split_docs.extend(split_docs)\n",
    "\n",
    "        return all_split_docs\n",
    "\n",
    "    def create_vectorstore(self):\n",
    "        embedding = OllamaEmbeddings(model=self.embedding_model)\n",
    "        return FAISS.from_documents(self.docs, embedding)\n",
    "\n",
    "    def create_multi_query_retriever(self):\n",
    "        return MultiQueryRetriever.from_llm(\n",
    "            retriever=self.db.as_retriever(),\n",
    "            llm=self.llm,\n",
    "            include_original=True\n",
    "        )\n",
    "\n",
    "    def ask(self, question: str) -> str:\n",
    "        return self.qa_chain.run(question)\n",
    "qa = MultiPromptQA(\n",
    "    file_paths=[\n",
    "        \"C:/Users/Akshaya V/git/Earnings research/Earnings_agent/citi.txt\",\n",
    "        \"C:/Users/Akshaya V/git/Earnings research/Earnings_agent/jpmc.txt\"\n",
    "    ],\n",
    "    embedding_model=\"qwen2.5:0.5b\",\n",
    "    llm_model=\"llama3.2:3b\"\n",
    ")\n",
    "\n",
    "response = qa.ask(\"Compare the EPS of Citi and JPMorgan in 2025Q1.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156fc57d-56a1-46eb-ad27-8adfb6c530c3",
   "metadata": {},
   "outputs": [],
   "source": [
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
